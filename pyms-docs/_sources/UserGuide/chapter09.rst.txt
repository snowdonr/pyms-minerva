******************************
Parallel processing with PyMS
******************************

.. contents:: Table of Contents

Requirements
==============

.. Caution:: The following markup has not been rewritten. Proceed with Caution

Using PyMS parallel capabilities requires installation of the package
'mpi4py', which provides bindings of the Message Passing Interface (MPI)
for the Python programming language. Installation instructions can be found in
`here <chapter01.html#package-mpi4py-required-for-parallel-processing>`__.

Background to using PyMS in parallel
=========================================

Any processing that loops through ion chromatograms or mass spectra can
be performed in parallel, by distributing the processing of individual
ion chromatograms or mass spectra to different CPUs by using the
efficient MPI mechanism.

Before the parallel processing can be deployed, data needs to be binned
to produce an :py:meth:`IntensityMatrix <pyms.GCMS.Class.IntensityMatrix>`
object, as described in the Section
"`IntensityMatrix Object <chapter04.html#intensitymatrix-object>`__".
This is essentially a two dimensional matrix, with ion chromatograms
along one dimension and mass spectra along the other dimension.

Consider the processing which applies a noise smoothing function to
each ion chromatogram. We first read the raw data:

.. code-block:: python

    andi_file = "data/gc01_0812_066.cdf"
    data = ANDI_reader(andi_file)

Then build the intensity matrix, and get its dimensions:

.. code-block:: python

    im = build_intensity_matrix_i(data)
    n_scan, n_mz = im.get_size()

The last command sets the variables n_scan and n_mz to the number
scans and number of :math:`m/z` values present in data, respectively.
Processing of ion chromatograms with the noise smoothing function
requires fetching of each ion chromatogram from the data, and
application of the noise smoothing function. This can be achieved
with a simple loop:

.. code-block:: python

    for ii in n_mz:
        print ii+1,
        ic = im.get_ic_at_index(ii)
        ic_smooth = window_smooth(ic, window=7)

This example epitomizes the typical processing required on the
GC-MS data. Another, equally important processing, is that of
individual mass spectra. In this case the same logic can be
applied, except that one would loop over the other dimension
of the IntensityMatrix object 'im'. That is, one would loop
over all the scan indices, and use the method
get_ms_at_index() to fetch individual mass spectra:

.. code-block:: python

    for ii in n_scan:
        print ii+1,
        ms = im.get_ms_at_index(ii)
        # here do something the the mass spectra 'ms'

Processing of data in this fashion is computationally intensive.
A typical data set may consist of 3,000-10,000 scans and ~500
:math:`m/z` values. If complex processing algorithms are applied to
each ion chromatogram (or mass spectra), the processing will
quickly become computationally prohibitive.

The type of calculation illustrated above is an ideal candidate
for parallelization because each ion chromatogram (or mass
spectrum) are processed independently. PyMS takes advantage
of this and allows one to harvest the power of multiple CPUs
to speed-up the processing. To achieve this PyMS can distributes
the loop from the above (either type, ie. over ion chromatograms
or mass spectra) over the available CPUs, achieving a linear
speed-up with the number of CPUs.


Using PyMS in parallel
========================

Using PyMS in parallel requires a minimal intervention, only
that special method of the IntensityMatrix object is invoked
in the for loop described above. For looping over all ion
chromatograms in parallel,

.. code-block:: python

    for ii in im.iter_ic_indices():
        print ii+1,
        ic = im.get_ic_at_index(ii)
        ic_smooth = window_smooth(ic, window=7)

The only change is that:

.. code-block:: python

    for ii in n_mz:

is replaced with:

.. code-block:: python

    for ii in im.iter_ic_indices()


The corresponding method for looping over all mass spectra
would involve replacing:

.. code-block:: python

    for ii in n_scan:

with:

.. code-block:: python

    for ii in im.iter_ms_indices()

The special constructs ``for ii in im.iter_ic_indices():`` and
``for ii in im.iter_ms_indices()`` will distribute the calculation
in parallel if MPI capability is available (ie. mpi4py is installed
on the system, and multiple CPUs are available). If MPI capability
is not available, the processing will be performed in a serial mode.
Running in parallel also requires some prior preparations, as explained
below.

Consider how the following script that performs noise smoothing example
described above (named 'proc.py'). This script is can be run in serial
or parallel mode.

.. code-block:: python
   :linenos:

    """proc.py
    """

    import sys
    sys.path.append("/x/PyMS")

    from pyms.GCMS.IO.ANDI.Function import ANDI_reader
    from pyms.GCMS.Function import build_intensity_matrix_i
    from pyms.Noise.Window import window_smooth

    # read the raw data as a GCMS_data object
    andi_file = "data/gc01_0812_066.cdf"
    data = ANDI_reader(andi_file)

    # build the intensity matrix
    im = build_intensity_matrix_i(data)

    # get the size of the intensity matrix
    n_scan, n_mz = im.get_size()
    print "Size of the intensity matrix is (n_scans, n_mz):", n_scan, n_mz

    # loop over all m/z values, fetch the corresponding IC, and perform
    # noise smoothing
    for ii in im.iter_ic_indices():
        print ii+1,
        ic = im.get_ic_at_index(ii)
        ic_smooth = window_smooth(ic, window=7)


A simple running of this script will produce a serial run without any warning messages:

.. code-block:: bash

    $ python3 proc.py
     -> Reading netCDF file 'data/gc01_0812_066.cdf'
    Size of the intensity matrix is (n_scans, n_mz): 9865 551
    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
    26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
    ... [ further output deleted ] ...

Inspection of the CPU usage during the execution of the program
shows that only one CPU is utilised 100\% (although multiple CPUs
are available) as shown here:

.. figure:: graphics/chapter09/top-serial.png
    :alt: The xterm output of the program 'top' with PyMS running in serial mode on the computer with multiple CPUs

    The xterm output of the program 'top' with PyMS running in serial mode on the computer with multiple CPUs

To run the above script in parallel, one needs first to start the
mpich process launcher, called 'mpd' (this is a program, in this
example located in ``/usr/bin/mpd``) This can be achieved as follows:

.. code-block:: bash

    $ /usr/bin/mpd --daemon

The above command starts 'mpd' as a daemon (the program runs in the
background, without a controlling terminal).  A common problem that
causes the above command is fail is the absence of the file ``.mpd.conf``
which 'mpd' requires to be present in the home directory of the user
who is starting the process. Here is an excerpt from the 'mpd' help page:

.. code-block:: text

   A file named .mpd.conf file must be present in the user's home directory
   with read and write access only for the user, and must contain at least
   a line with MPD_SECRETWORD=<secretword>
   To run mpd as root, install it while root and instead of a .mpd.conf file
   use mpd.conf (no leading dot) in the /etc directory.'

Fixing this problem is simple, and requires creating the file
``~/.mpd.conf``, which in our case contains only one line:

.. code-block:: bash

    MPD_SECRETWORD=euSe0veo

After this the 'mpd' can be launched. Running the PyMS script in the parallel
mode requires the use of 'mpirun' command,

.. code-block:: bash

    $ /usr/bin/mpirun -np 2 python proc.py

The above command prepare 'python proc.py' to run in parallel, in this
case by using two CPUS (``-np 2``). The execution produces the following output:

.. code-block:: bash

    $ /usr/bin/mpirun -np 2 python proc.py
     -> Reading netCDF file 'data/gc01_0812_066.cdf'
     -> Reading netCDF file 'data/gc01_0812_066.cdf'
    Size of the intensity matrix is (n_scans, n_mz):Size of
    the intensity matrix is (n_scans, n_mz): 9865 551
    276 9865 551
    1 277 2 278 3 279 4 280 5 281 6 282 7 283 8 284 9 285 10 286 11 287 12
    288 13 289 14 290 15 291 16 292 17 293 18 294 19 295 20 296 21 297 22
    298 23 299 24 300 25 301 26 302 27 303 28 304 2
    ... [ further output deleted ] ...

The above shows that two processes are active (each reading its own
version of data). While the distribution of processing between the
two processes has been achieved automatically by PyMS. Since both
processes were started from the same terminal their output is
intermingled. This time the processing is using two CPUs, and this
can be seen from the inspection of CPU utilisation, as shown below.
Also the execution of the script 'proc.py' is now two times faster.

.. figure:: graphics/chapter09/top-parallel.png
    :alt: The xterm output of the program 'top' with PyMS running in parallel mode with two CPUs

    The xterm output of the program 'top' with PyMS running in parallel mode with two CPUs

This simple example shows how to speed-up PyMS processing on a common
workstation with two CPUs. The MPI also allows PyMS processing to run
on specialised computers with many CPUs, such as Linux clusters. The
MPI implementation allows PyMS to be easily used in such distributed
computing environments, much like in the example above. We have tested
PyMS on Linux clusters, and the resulting speed-up is nearly linear
with the number of CPUs employed.

.. figure:: graphics/chapter09/speedup-parallel.png
    :alt: The speedup in PyMS processing when run in parallel on a Linux cluster as a function of a number of CPUs deployed

    The speedup in PyMS processing when run in parallel on a Linux cluster as a function of a number of CPUs deployed
